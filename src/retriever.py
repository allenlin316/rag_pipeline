import requests
import chromadb
from chromadb.config import Settings
from typing import List, Dict, Any
try:
    from tqdm import tqdm  # å¯é¸ä¾è³´ï¼šè‹¥ä¸å­˜åœ¨å‰‡é€€å›ç°¡å–®åˆ—å°
except Exception:
    tqdm = None
from dataclasses import dataclass
from config import get_config
from .text_chunker import DocumentChunker, TextChunker
from .text_splitters import Language

@dataclass
class Document:
    """æ–‡ä»¶è³‡æ–™çµæ§‹"""
    content: str
    metadata: Dict[str, Any] = None
    score: float = 0.0

class EmbeddingAPI:
    """Embedding API å®¢æˆ¶ç«¯"""
    def __init__(self, api_key: str = None, base_url: str = None, model: str = None):
        config = get_config()
        self.api_key = config.generator_api_key
        self.base_url = base_url or config.retriever_base_url
        self.model = model or config.embedding_model
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        self.session = requests.Session()
    
    def get_embedding(self, text: str, model: str = None) -> List[float]:
        """ç²å–æ–‡æœ¬çš„ embedding"""
        if not text or not text.strip():
            raise Exception("æ–‡æœ¬å…§å®¹ç‚ºç©º")
        
        data = {
            "input": text,
            "model": model or self.model
        }
        
        try:
            r = self.session.post(
                f"{self.base_url}/embeddings",
                headers=self.headers,
                json=data,
                timeout=30  # æ·»åŠ è¶…æ™‚
            )
            r.raise_for_status()
            result = r.json()
            if "data" in result and len(result["data"]) > 0:
                embedding = result["data"][0]["embedding"]
                if embedding and len(embedding) > 0:
                    return embedding
                else:
                    raise Exception("API è¿”å›çš„ embedding ç‚ºç©º")
            else:
                raise Exception("API è¿”å›çš„æ•¸æ“šæ ¼å¼ä¸æ­£ç¢º")
                
        except requests.exceptions.RequestException as e:
            raise Exception(f"ç¶²çµ¡è«‹æ±‚éŒ¯èª¤: {e}")
        except Exception as e:
            raise Exception(f"ç²å– embedding æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    def get_embeddings(self, texts: List[str], model: str = None) -> List[List[float]]:
        """ç²å–å¤šå€‹æ–‡æœ¬çš„ embedding"""
        if not texts:
            return []
        data = {
            "input": texts,
            "model": model or self.model
        }
        try:
            r = self.session.post(
                f"{self.base_url}/embeddings",
                headers=self.headers,
                json=data,
                timeout=60  # æ·»åŠ è¶…æ™‚
            )
            r.raise_for_status()
            result = r.json()
            if "data" in result and len(result["data"]) > 0:
                return [d["embedding"] for d in result["data"]]
            else:
                raise Exception("API è¿”å›çš„æ•¸æ“šæ ¼å¼ä¸æ­£ç¢º")
        except requests.exceptions.RequestException as e:
            raise Exception(f"ç¶²çµ¡è«‹æ±‚éŒ¯èª¤: {e}")
        except Exception as e:
            raise Exception(f"ç²å– embeddings æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

class ChromaVectorStore:
    """ä½¿ç”¨ Chroma çš„å‘é‡å„²å­˜"""
    def __init__(
        self, 
        collection_name: str = "rag_documents", 
        persist_directory: str = "./chroma_db",
        enable_chunking: bool = True,
        chunk_size: int = 512,
        chunk_overlap: int = 0,
        use_recursive: bool = True,
        language: Language = None,
        reset: bool = False
    ):
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        self.enable_chunking = enable_chunking
        
        # åˆå§‹åŒ– Chroma å®¢æˆ¶ç«¯
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # é‡ç½®é›†åˆï¼ˆå¯é¸ï¼‰
        if reset:
            try:
                self.client.delete_collection(name=collection_name)
                print(f"ğŸ—‘ï¸ å·²é‡ç½®é›†åˆ: {collection_name}")
            except Exception:
                pass

        # ç²å–æˆ–å‰µå»ºé›†åˆ
        try:
            self.collection = self.client.get_collection(name=collection_name)
            print(f"âœ… è¼‰å…¥ç¾æœ‰é›†åˆ: {collection_name}")
        except:
            self.collection = self.client.create_collection(
                name=collection_name,
                metadata={"description": "RAG Pipeline æ–‡ä»¶é›†åˆ"}
            )
            print(f"âœ… å‰µå»ºæ–°é›†åˆ: {collection_name}")
        
        # Embedding API å®¢æˆ¶ç«¯
        self.embedding_api = EmbeddingAPI()
        
        # æ–‡æœ¬åˆ†å¡Šå™¨
        if self.enable_chunking:
            self.chunker = DocumentChunker(
                TextChunker(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap,
                    use_recursive=use_recursive,
                    language=language
                )
            )
            chunker_type = "éæ­¸åˆ†å‰²å™¨" if use_recursive else "å­—ç¬¦åˆ†å‰²å™¨"
            language_info = f" ({language.value})" if language else ""
            print(f"âœ… å•Ÿç”¨æ–‡æœ¬åˆ†å¡Š: {chunker_type}{language_info} (chunk_size={chunk_size}, overlap={chunk_overlap})")
        else:
            self.chunker = None
            print("â­ï¸ ç¦ç”¨æ–‡æœ¬åˆ†å¡Š")
        
        print(f"ä½¿ç”¨ {self.embedding_api.model} é€²è¡Œ embedding")
        
    def add_documents(self, documents: List[Document]):
        """æ·»åŠ æ–‡ä»¶åˆ° Chroma å‘é‡å„²å­˜"""
        print(f"ğŸ“š æ·»åŠ  {len(documents)} å€‹æ–‡ä»¶åˆ° Chroma...")
        
        # å¦‚æœå•Ÿç”¨åˆ†å¡Šï¼Œå…ˆé€²è¡Œæ–‡æœ¬åˆ†å¡Š
        if self.enable_chunking and self.chunker:
            print("ğŸ”„ é€²è¡Œæ–‡æœ¬åˆ†å¡Š...")
            chunked_documents = self.chunker.chunk_documents(documents)
            print(f"ğŸ“„ åˆ†å¡Šå¾Œå¾—åˆ° {len(chunked_documents)} å€‹æ–‡æœ¬å¡Š")
            # è‹¥åˆ†å¡Šçµæœç‚ºç©ºï¼Œå›é€€ç‚ºåŸå§‹æ–‡æª”
            documents_to_add = chunked_documents or documents
        else:
            documents_to_add = documents
        
        # æº–å‚™æ•¸æ“š
        ids = []
        texts = []
        metadatas = []
        embeddings = []
        
        # å…ˆå˜—è©¦ç²å–ä¸€å€‹ embedding ä¾†ç¢ºå®šç¶­åº¦
        embedding_dimension = None
        try:
            test_embedding = self.embedding_api.get_embedding("test")
            embedding_dimension = len(test_embedding)
            print(f"âœ… ç¢ºèª embedding ç¶­åº¦: {embedding_dimension}")
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•ç¢ºå®š embedding ç¶­åº¦: {e}")
            embedding_dimension = 1536  # é è¨­ç¶­åº¦
        
        total_docs = len(documents_to_add)
        iterator = documents_to_add
        if tqdm is not None:
            iterator = tqdm(documents_to_add, desc="Embedding documents", unit="doc")

        for i, doc in enumerate(iterator):
            # ç”Ÿæˆå”¯ä¸€ ID - ä½¿ç”¨æ›´å¯é çš„æ–¹å¼
            import uuid
            doc_id = f"doc_{i}_{uuid.uuid4().hex[:8]}"
            ids.append(doc_id)
            
            # æ–‡æœ¬å…§å®¹
            texts.append(doc.content)
            
            # å…ƒæ•¸æ“š
            metadata = doc.metadata or {}
            metadata.update({
                "source": "rag_pipeline",
                "content_length": len(doc.content),
                "is_chunked": self.enable_chunking
            })
            metadatas.append(metadata)
            
            # ç²å– embedding
            try:
                embedding = self.embedding_api.get_embedding(doc.content)
                if embedding and len(embedding) > 0:
                    embeddings.append(embedding)
                else:
                    raise Exception("ç²å–åˆ°çš„ embedding ç‚ºç©º")
            except Exception as e:
                print(f"âš ï¸ ç²å– embedding å¤±æ•— (æ–‡æª” {i+1}): {e}")
                # å¦‚æœ API å¤±æ•—ï¼Œä½¿ç”¨é›¶å‘é‡ä½œç‚ºå‚™ç”¨
                zero_embedding = [0.0] * embedding_dimension
                embeddings.append(zero_embedding)

            # ç°¡æ˜“é€²åº¦è¼¸å‡ºï¼ˆç•¶æœªå®‰è£ tqdm æ™‚ï¼‰
            if tqdm is None and total_docs > 0:
                if (i + 1) == 1 or (i + 1) % 5 == 0 or (i + 1) == total_docs:
                    print(f"â³ é€²åº¦: {i + 1}/{total_docs}")
        
        # è‹¥æ²’æœ‰å¯æ·»åŠ çš„æ–‡æª”ï¼Œç›´æ¥è¿”å›
        if not ids and len(documents_to_add) == 0:
            print("âš ï¸ æ²’æœ‰å¯æ·»åŠ çš„æ–‡ä»¶ï¼Œè·³é")
            return

        # æ‰¹é‡æ·»åŠ åˆ° Chroma
        try:
            if not ids or not embeddings:
                print("âš ï¸ ç©ºçš„ ids æˆ– embeddingsï¼Œè·³éæ·»åŠ ")
                return
            
            # ä½¿ç”¨æ‰¹æ¬¡ API é€²è¡ŒåµŒå…¥ï¼ˆä¿®æ­£ç‰ˆæœ¬ï¼‰
            batch_size = 64 # è¨­å®šæ‰¹æ¬¡å¤§å°
            final_embeddings = []
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                batch_ids = ids[i:i+batch_size]
                batch_metadatas = metadatas[i:i+batch_size]
                
                batch_embs = self.embedding_api.get_embeddings(batch_texts)
                
                if len(batch_embs) != len(batch_texts):
                    raise Exception(f"æ‰¹æ¬¡åµŒå…¥å¤±æ•—ï¼Œé æœŸ {len(batch_texts)} å€‹ embeddingï¼Œä½†åªæ”¶åˆ° {len(batch_embs)} å€‹")
                
                for j, emb in enumerate(batch_embs):
                    if emb and len(emb) > 0:
                        final_embeddings.append(emb)
                    else:
                        print(f"âš ï¸ æ‰¹æ¬¡åµŒå…¥å¤±æ•— (æ–‡æª” {i+j+1}): ç²å–åˆ°çš„ embedding ç‚ºç©º")
                        # å¦‚æœ API å¤±æ•—ï¼Œä½¿ç”¨é›¶å‘é‡ä½œç‚ºå‚™ç”¨
                        final_embeddings.append([0.0] * embedding_dimension)

            self.collection.add(
                ids=ids,
                documents=texts,
                metadatas=metadatas,
                embeddings=final_embeddings
            )
            print(f"âœ… æˆåŠŸæ·»åŠ  {len(documents_to_add)} å€‹æ–‡ä»¶åˆ° Chroma")
        except Exception as e:
            print(f"âŒ æ·»åŠ æ–‡ä»¶åˆ° Chroma å¤±æ•—: {e}")
            raise
    
    def search(self, query: str, top_k: int = 10) -> List[Document]:
        """æœå°‹ç›¸ä¼¼æ–‡ä»¶"""
        try:
            # ç²å–æŸ¥è©¢çš„ embedding
            query_embedding = self.embedding_api.get_embedding(query)
            
            # åœ¨ Chroma ä¸­æœå°‹
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                include=["documents", "metadatas", "distances"]
            )
            
            # è½‰æ›ç‚º Document ç‰©ä»¶ä¸¦æŒ‰åˆ†æ•¸æ’åº
            all_documents = []
            
            if results["documents"] and results["documents"][0]:
                print(f"ğŸ” æª¢ç´¢åˆ° {len(results['documents'][0])} å€‹æ–‡æª”")
                print(f"ğŸ” è·é›¢å€¼: {results['distances'][0]}")
                
                # å…ˆè™•ç†æ‰€æœ‰æ–‡æª”ï¼Œè¨ˆç®—åˆ†æ•¸
                for i, (doc_text, metadata, distance) in enumerate(zip(
                    results["documents"][0],
                    results["metadatas"][0],
                    results["distances"][0]
                )):
                    # å°‡è·é›¢è½‰æ›ç‚ºç›¸ä¼¼åº¦åˆ†æ•¸ (Chroma ä½¿ç”¨æ­å¹¾é‡Œå¾—è·é›¢)
                    # ä½¿ç”¨æ›´å¥½çš„è½‰æ›æ–¹æ³•ï¼š1 - normalized_distance
                    if len(results["distances"][0]) > 1:
                        max_distance = max(results["distances"][0])
                        min_distance = min(results["distances"][0])
                        if max_distance > min_distance:
                            normalized_distance = (distance - min_distance) / (max_distance - min_distance)
                            similarity_score = 1.0 - normalized_distance
                        else:
                            similarity_score = 1.0  # å¦‚æœæ‰€æœ‰è·é›¢éƒ½ç›¸åŒ
                    else:
                        # å¦‚æœåªæœ‰ä¸€å€‹çµæœï¼Œä½¿ç”¨æ›´åˆç†çš„è½‰æ›æ–¹æ³•
                        similarity_score = max(0.0, 1.0 - (distance / 10.0))  # å‡è¨­æœ€å¤§è·é›¢ç‚º10
                    
                    doc = Document(
                        content=doc_text,
                        metadata=metadata,
                        score=similarity_score
                    )
                    all_documents.append(doc)
                    print(f"ğŸ” æ–‡æª” {i+1} åˆ†æ•¸: {similarity_score:.6f} (è·é›¢: {distance:.6f})")
                
                # æŒ‰åˆ†æ•¸æ’åºï¼ˆé™åºï¼‰
                all_documents.sort(key=lambda x: x.score, reverse=True)
                print(f"ğŸ“Š æŒ‰åˆ†æ•¸æ’åºå®Œæˆï¼Œæœ€é«˜åˆ†: {all_documents[0].score:.6f}, æœ€ä½åˆ†: {all_documents[-1].score:.6f}")
                
                # å»é‡ä¸¦ä¿ç•™æœ€é«˜åˆ†çš„æ–‡æª”
                unique_documents = []
                seen_contents = set()
                
                for doc in all_documents:
                    # æª¢æŸ¥æ˜¯å¦ç‚ºå®Œå…¨é‡è¤‡
                    if doc.content in seen_contents:
                        print(f"ğŸ”„ è·³éé‡è¤‡å…§å®¹ (åˆ†æ•¸: {doc.score:.6f})")
                        continue
                    
                    # æª¢æŸ¥æ˜¯å¦ç‚º overlap é‡è¤‡ï¼ˆå¦‚æœå•Ÿç”¨äº†åˆ†å¡Šä¸”æœ‰ overlapï¼‰
                    is_overlap_duplicate = False
                    if self.enable_chunking and hasattr(self, 'chunker') and self.chunker:
                        for seen_content in seen_contents:
                            # å¦‚æœä¸€å€‹æ–‡æœ¬å®Œå…¨åŒ…å«åœ¨å¦ä¸€å€‹æ–‡æœ¬ä¸­ï¼Œå¯èƒ½æ˜¯ overlap é‡è¤‡
                            if (len(doc.content) > len(seen_content) and seen_content in doc.content) or \
                               (len(seen_content) > len(doc.content) and doc.content in seen_content):
                                # è¨ˆç®—é‡ç–Šæ¯”ä¾‹
                                overlap_ratio = min(len(doc.content), len(seen_content)) / max(len(doc.content), len(seen_content))
                                if overlap_ratio > 0.8:  # å¦‚æœé‡ç–Šè¶…é80%ï¼Œè¦–ç‚º overlap é‡è¤‡
                                    print(f"ğŸ”„ è·³é overlap é‡è¤‡ (åˆ†æ•¸: {doc.score:.6f}, é‡ç–Šæ¯”ä¾‹: {overlap_ratio:.2f})")
                                    is_overlap_duplicate = True
                                    break
                    
                    if is_overlap_duplicate:
                        continue
                    
                    seen_contents.add(doc.content)
                    unique_documents.append(doc)
                    print(f"âœ… ä¿ç•™æ–‡æª” (åˆ†æ•¸: {doc.score:.6f})")
                
                # è¿”å›å‰ top_k å€‹å”¯ä¸€æ–‡æª”
                final_documents = unique_documents[:top_k]
                print(f"âœ… å»é‡å¾Œè¿”å› {len(final_documents)} å€‹å”¯ä¸€æ–‡æª” (æœ€é«˜åˆ†: {final_documents[0].score:.6f})")
                return final_documents
            
        except Exception as e:
            print(f"âŒ Chroma æœå°‹å¤±æ•—: {e}")
            return []
    
    def get_collection_info(self) -> Dict[str, Any]:
        """ç²å–é›†åˆè³‡è¨Š"""
        try:
            count = self.collection.count()
            return {
                "collection_name": self.collection_name,
                "document_count": count,
                "persist_directory": self.persist_directory
            }
        except Exception as e:
            print(f"âŒ ç²å–é›†åˆè³‡è¨Šå¤±æ•—: {e}")
            return {}
    
    def delete_collection(self):
        """åˆªé™¤é›†åˆ"""
        try:
            self.client.delete_collection(name=self.collection_name)
            print(f"âœ… åˆªé™¤é›†åˆ: {self.collection_name}")
        except Exception as e:
            print(f"âŒ åˆªé™¤é›†åˆå¤±æ•—: {e}")

def retriever(query: str, vector_store: ChromaVectorStore, top_k: int = 20) -> List[Document]:
    """
    Retriever å‡½æ•¸ï¼šå¾å‘é‡å„²å­˜ä¸­æª¢ç´¢ç›¸é—œæ–‡ä»¶
    
    Args:
        query: æŸ¥è©¢æ–‡æœ¬
        vector_store: å‘é‡å„²å­˜å¯¦ä¾‹
        top_k: è¿”å›çš„æ–‡ä»¶æ•¸é‡
    
    Returns:
        ç›¸é—œæ–‡ä»¶åˆ—è¡¨
    """
    print(f"ğŸ” æª¢ç´¢æŸ¥è©¢: {query}")
    documents = vector_store.search(query, top_k=top_k)
    print(f"ğŸ“„ æª¢ç´¢åˆ° {len(documents)} å€‹æ–‡ä»¶")
    return documents

# ç‚ºäº†å‘å¾Œå…¼å®¹ï¼Œä¿ç•™ VectorStore åˆ¥å
VectorStore = ChromaVectorStore
